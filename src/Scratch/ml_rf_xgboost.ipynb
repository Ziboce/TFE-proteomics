{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80130,
     "status": "ok",
     "timestamp": 1739445716874,
     "user": {
      "displayName": "Jimmy Walraff",
      "userId": "17365452855707799727"
     },
     "user_tz": -60
    },
    "id": "ILuRLHcMLz4L",
    "outputId": "69695f2b-4a27-46bb-8208-4ed1f7952f8b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/Walraff/OneDrive - Universite de Liege/Documents/Ulg/Master2/TFE/\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1739445719023,
     "user": {
      "displayName": "Jimmy Walraff",
      "userId": "17365452855707799727"
     },
     "user_tz": -60
    },
    "id": "e4ssIbG_Lz4a",
    "outputId": "8f6ffff3-ba71-486a-ea59-5fae49038f08"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProteinName_SPARE</th>\n",
       "      <th>Peptide_SPARE</th>\n",
       "      <th>Status_SPARE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sp|P02751|FINC_HUMAN</td>\n",
       "      <td>VDVIPVNLPGEHGQR</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sp|P02751|FINC_HUMAN</td>\n",
       "      <td>STTPDITGYR</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sp|P02751|FINC_HUMAN</td>\n",
       "      <td>SYTITGLQPGTDYK</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sp|P02751|FINC_HUMAN</td>\n",
       "      <td>IYLYTLNDNAR</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sp|P04114|APOB_HUMAN</td>\n",
       "      <td>TGISPLALIK</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>sp|P02743|SAMP_HUMAN</td>\n",
       "      <td>VGEYSLYIGR</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>sp|P04004|VTNC_HUMAN</td>\n",
       "      <td>GQYCYELDEK</td>\n",
       "      <td>mauvais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>sp|P04004|VTNC_HUMAN</td>\n",
       "      <td>FEDGVLDPDYPR</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>sp|P04004|VTNC_HUMAN</td>\n",
       "      <td>DWHGVPGQVDAAMAGR</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>sp|P04004|VTNC_HUMAN</td>\n",
       "      <td>SIAQYWLGCPAPGHL</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ProteinName_SPARE     Peptide_SPARE Status_SPARE\n",
       "0    sp|P02751|FINC_HUMAN   VDVIPVNLPGEHGQR          bon\n",
       "1    sp|P02751|FINC_HUMAN        STTPDITGYR          bon\n",
       "2    sp|P02751|FINC_HUMAN    SYTITGLQPGTDYK          bon\n",
       "3    sp|P02751|FINC_HUMAN       IYLYTLNDNAR          bon\n",
       "4    sp|P04114|APOB_HUMAN        TGISPLALIK          bon\n",
       "..                    ...               ...          ...\n",
       "150  sp|P02743|SAMP_HUMAN        VGEYSLYIGR          bon\n",
       "151  sp|P04004|VTNC_HUMAN        GQYCYELDEK      mauvais\n",
       "152  sp|P04004|VTNC_HUMAN      FEDGVLDPDYPR          bon\n",
       "153  sp|P04004|VTNC_HUMAN  DWHGVPGQVDAAMAGR          bon\n",
       "154  sp|P04004|VTNC_HUMAN   SIAQYWLGCPAPGHL          bon\n",
       "\n",
       "[155 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:/Users/Walraff/OneDrive - Universite de Liege/Documents/Ulg/Master2/TFE/data\"\n",
    "# data_path = \"/content/drive/MyDrive/TFE\"\n",
    "original_df = pd.read_csv(f'{data_path}/final_status_SPARE.csv')\n",
    "original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1739445719025,
     "user": {
      "displayName": "Jimmy Walraff",
      "userId": "17365452855707799727"
     },
     "user_tz": -60
    },
    "id": "EL4z6y7sLz4l",
    "outputId": "0f741b44-40e2-4793-8195-52810c88976d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantotypic\n",
      "0    117\n",
      "1     38\n",
      "Name: count, dtype: int64\n",
      "3.0789473684210527\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe with the sequences and labels\n",
    "df = pd.DataFrame()\n",
    "df[\"sequence\"] = original_df[\"Peptide_SPARE\"]\n",
    "df[\"quantotypic\"] = original_df.apply(lambda row: 0 if row['Status_SPARE'] == 'bon' else 1, axis=1)\n",
    "\n",
    "positive_df = df[df['quantotypic'] == 0]\n",
    "negative_df = df[df['quantotypic'] == 1]\n",
    "\n",
    "class_counts = df['quantotypic'].value_counts()\n",
    "max_len = df['sequence'].str.len().max()\n",
    "pos_weight = class_counts[0] / class_counts[1]\n",
    "print(class_counts)\n",
    "print(pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_sequence(sequence, max_len, vocab):\n",
    "    \"\"\"\n",
    "    Converts an amino acid sequence into a one-hot encoded matrix of shape (max_len, vocab_size).\n",
    "    Unused positions (due to padding) remain as zero rows.\n",
    "\n",
    "    Args:\n",
    "        sequence (str): Amino acid sequence.\n",
    "        max_len (int): Maximum sequence length (for padding/truncation).\n",
    "        vocab (dict): Mapping from amino acid to index (1-based).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: One-hot encoded matrix of shape (max_len, len(vocab)).\n",
    "    \"\"\"\n",
    "    n_vocab = len(vocab)\n",
    "    encoded = np.zeros((max_len, n_vocab), dtype=np.float32)\n",
    "\n",
    "    for i, aa in enumerate(sequence):\n",
    "        if i < max_len:\n",
    "            idx = vocab.get(aa, None)\n",
    "            if idx is not None:\n",
    "                encoded[i, idx - 1] = 1.0  # Convert to 0-based index\n",
    "\n",
    "    return encoded\n",
    "\n",
    "amino_acid_vocab = {aa: idx+1 for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
    "\n",
    "df['encoded_sequence'] = df['sequence'].apply(lambda seq: one_hot_encode_sequence(seq, max_len, amino_acid_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': 3, 'max_features': None}\n",
      "0.3836180150339142\n",
      "Accuracy: 0.8065\n",
      "Precision: 1.0000\n",
      "Recall: 0.1429\n",
      "F1 Score: 0.2500\n",
      "ROC AUC: 0.5714\n",
      "PR AUC: 0.3364\n",
      "Fold: 1\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': 1, 'max_features': 'log2'}\n",
      "0.3507694363231736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7742\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "ROC AUC: 0.5000\n",
      "PR AUC: 0.2258\n",
      "Fold: 2\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': 5, 'max_features': 'sqrt'}\n",
      "0.4503078580508248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7419\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "ROC AUC: 0.5000\n",
      "PR AUC: 0.2581\n",
      "Fold: 3\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': 1, 'max_features': None}\n",
      "0.4001824269641608\n",
      "Accuracy: 0.7742\n",
      "Precision: 0.6667\n",
      "Recall: 0.2500\n",
      "F1 Score: 0.3636\n",
      "ROC AUC: 0.6033\n",
      "PR AUC: 0.3602\n",
      "Fold: 4\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': None, 'max_features': None}\n",
      "0.47287215812983063\n",
      "Accuracy: 0.6774\n",
      "Precision: 0.2500\n",
      "Recall: 0.1250\n",
      "F1 Score: 0.1667\n",
      "ROC AUC: 0.4973\n",
      "PR AUC: 0.2571\n",
      "Fold: 0\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': 5, 'max_features': 'log2'}\n",
      "0.3578912737012895\n",
      "Accuracy: 0.7419\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "ROC AUC: 0.4792\n",
      "PR AUC: 0.2258\n",
      "Fold: 1\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': 3, 'max_features': 'log2'}\n",
      "0.3439316717606624\n",
      "Accuracy: 0.7742\n",
      "Precision: 0.5000\n",
      "Recall: 0.1429\n",
      "F1 Score: 0.2222\n",
      "ROC AUC: 0.5506\n",
      "PR AUC: 0.2650\n",
      "Fold: 2\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': 1, 'max_features': 'sqrt'}\n",
      "0.5119039448683411\n",
      "Accuracy: 0.6774\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "ROC AUC: 0.4565\n",
      "PR AUC: 0.2581\n",
      "Fold: 3\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': 1, 'max_features': None}\n",
      "0.40739917227321315\n",
      "Accuracy: 0.7742\n",
      "Precision: 0.6000\n",
      "Recall: 0.3750\n",
      "F1 Score: 0.4615\n",
      "ROC AUC: 0.6440\n",
      "PR AUC: 0.3863\n",
      "Fold: 4\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'max_depth': 1, 'max_features': None}\n",
      "0.47036904442780064\n",
      "Accuracy: 0.7419\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "ROC AUC: 0.5000\n",
      "PR AUC: 0.2581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "count = 1\n",
    "\n",
    "# Trying with and without weighting\n",
    "for weighting in [False, True]:\n",
    "    temp_result_list = []\n",
    "    # loop through the folds\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(df['encoded_sequence'], df['quantotypic'])):\n",
    "        # Splitting the data into train and test sets\n",
    "        train_df = df.iloc[train_idx]\n",
    "        test_df = df.iloc[test_idx]\n",
    "        print(f\"Fold: {fold}\")\n",
    "        print()\n",
    "        \n",
    "        # Prepare the data for training and testing\n",
    "        X_train = np.stack(train_df['encoded_sequence'].values)\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        y_train = train_df['quantotypic'].values\n",
    "\n",
    "        X_test = np.stack(test_df['encoded_sequence'].values)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "        y_test = test_df['quantotypic'].values\n",
    "\n",
    "        # Param grid for RandomForestClassifier\n",
    "        param_grid = {\n",
    "            'max_depth': [None, 1, 3, 5, 6],\n",
    "            'max_features': ['sqrt', 'log2', None], # None => max feature = n features\n",
    "        }\n",
    "\n",
    "        # if weighting is True, add class_weight to the classifier\n",
    "        if weighting:\n",
    "            rf = RandomForestClassifier(n_estimators=1000, random_state=42, class_weight='balanced')\n",
    "        else:\n",
    "            rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "\n",
    "        # Train the model using GridSearchCV\n",
    "        grid_search_rf = GridSearchCV(rf, param_grid, cv=5, scoring='average_precision', verbose=4, n_jobs=-1)\n",
    "        grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "        print(grid_search_rf.best_params_)\n",
    "        print(grid_search_rf.best_score_)\n",
    "\n",
    "        # Retrieve the best parameters\n",
    "        best_params = grid_search_rf.best_params_\n",
    "\n",
    "        # Initialize a new RandomForestClassifier with the best parameters\n",
    "        if weighting:\n",
    "            best_rf = RandomForestClassifier(n_estimators=1000,\n",
    "                **best_params,\n",
    "                random_state=42,\n",
    "                class_weight='balanced' \n",
    "            )\n",
    "        else:\n",
    "            best_rf = RandomForestClassifier(n_estimators=1000,\n",
    "                **best_params,\n",
    "                random_state=42 \n",
    "            )\n",
    "\n",
    "        # Retrain the model on the entire training set\n",
    "        best_rf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = best_rf.predict(X_test)\n",
    "\n",
    "        # Evaluate the model using the provided eval function\n",
    "        accuracy, precision, recall, f1, roc_auc, pr_auc = utils.eval(y_test, y_pred)\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"pr_auc\": pr_auc\n",
    "        }\n",
    "\n",
    "        temp_result_list.append(metrics)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "    accuracy_list = [result['accuracy'] for result in temp_result_list]\n",
    "    precision_list = [result['precision'] for result in temp_result_list]\n",
    "    recall_list = [result['recall'] for result in temp_result_list]\n",
    "    f1_list = [result['f1'] for result in temp_result_list]\n",
    "    roc_auc_list = [result['roc_auc'] for result in temp_result_list]\n",
    "    pr_auc_list = [result['pr_auc'] for result in temp_result_list]\n",
    "\n",
    "    metrics_summary = {\n",
    "        \"accuracy_mean\": np.mean(accuracy_list),\n",
    "        \"accuracy_std\": np.std(accuracy_list),\n",
    "        \"precision_mean\": np.mean(precision_list),\n",
    "        \"precision_std\": np.std(precision_list),\n",
    "        \"recall_mean\": np.mean(recall_list),\n",
    "        \"recall_std\": np.std(recall_list),\n",
    "        \"f1_mean\": np.mean(f1_list),\n",
    "        \"f1_std\": np.std(f1_list),\n",
    "        \"roc_auc_mean\": np.mean(roc_auc_list),\n",
    "        \"roc_auc_std\": np.std(roc_auc_list),\n",
    "        \"pr_auc_mean\": np.mean(pr_auc_list),\n",
    "        \"pr_auc_std\": np.std(pr_auc_list)\n",
    "    }\n",
    "\n",
    "    dict_save = {\n",
    "        \"weighting\": weighting,\n",
    "        \"oversampling\": False,\n",
    "        \"metrics_summary\": metrics_summary\n",
    "    }\n",
    "\n",
    "    path = \"C:/Users/Walraff/OneDrive - Universite de Liege/Documents/Ulg/Master2/TFE/Results/ValidProtocol\"\n",
    "    utils.write_into_json(dict_save, f\"{path}/rf_scratch_experiment_{count}.json\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'classifier__max_depth': 1, 'classifier__max_features': None}\n",
      "0.3651302866753802\n",
      "Accuracy: 0.8387\n",
      "Precision: 0.7500\n",
      "Recall: 0.4286\n",
      "F1 Score: 0.5455\n",
      "ROC AUC: 0.6935\n",
      "PR AUC: 0.4505\n",
      "Fold: 1\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'classifier__max_depth': 1, 'classifier__max_features': 'sqrt'}\n",
      "0.3830598013400399\n",
      "Accuracy: 0.6774\n",
      "Precision: 0.2000\n",
      "Recall: 0.1429\n",
      "F1 Score: 0.1667\n",
      "ROC AUC: 0.4881\n",
      "PR AUC: 0.2221\n",
      "Fold: 2\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'classifier__max_depth': 1, 'classifier__max_features': None}\n",
      "0.4569403037050096\n",
      "Accuracy: 0.4194\n",
      "Precision: 0.2500\n",
      "Recall: 0.6250\n",
      "F1 Score: 0.3571\n",
      "ROC AUC: 0.4864\n",
      "PR AUC: 0.2530\n",
      "Fold: 3\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'classifier__max_depth': 1, 'classifier__max_features': None}\n",
      "0.4055869185280949\n",
      "Accuracy: 0.7097\n",
      "Precision: 0.4444\n",
      "Recall: 0.5000\n",
      "F1 Score: 0.4706\n",
      "ROC AUC: 0.6413\n",
      "PR AUC: 0.3513\n",
      "Fold: 4\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "{'classifier__max_depth': 6, 'classifier__max_features': None}\n",
      "0.4404330682504057\n",
      "Accuracy: 0.5806\n",
      "Precision: 0.2727\n",
      "Recall: 0.3750\n",
      "F1 Score: 0.3158\n",
      "ROC AUC: 0.5136\n",
      "PR AUC: 0.2636\n"
     ]
    }
   ],
   "source": [
    "# New loop with oversampling using ImbPipeline\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "temp_result_list = []\n",
    "# loop over folds\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(df['encoded_sequence'], df['quantotypic'])):\n",
    "    # Splitting the data into train and test sets\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    print(f\"Fold: {fold}\")\n",
    "    print()\n",
    "\n",
    "    # Prepare the data for training and testing\n",
    "    X_train = np.stack(train_df['encoded_sequence'].values)\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    y_train = train_df['quantotypic'].values\n",
    "\n",
    "    X_test = np.stack(test_df['encoded_sequence'].values)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    y_test = test_df['quantotypic'].values\n",
    "\n",
    "    # Param grid for RandomForestClassifier\n",
    "    param_grid = {\n",
    "    'classifier__max_depth': [None, 1, 3, 5, 6],\n",
    "    'classifier__max_features': ['sqrt', 'log2', None],  # None = utiliser toutes les features\n",
    "    }\n",
    "\n",
    "    # Create a pipeline with RandomOverSampler and RandomForestClassifier\n",
    "    pipeline = ImbPipeline([\n",
    "        ('oversampler', RandomOverSampler(random_state=42)), \n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=1000, \n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Training the model using GridSearchCV\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,  \n",
    "        scoring='average_precision',\n",
    "        verbose=4, \n",
    "        n_jobs=-1 \n",
    "    )\n",
    "\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "    print(grid_search_rf.best_params_)\n",
    "    print(grid_search_rf.best_score_)\n",
    "\n",
    "    # Retrieve the best parameters\n",
    "    best_params = grid_search_rf.best_params_\n",
    "\n",
    "    best_rf = RandomForestClassifier(\n",
    "        n_estimators=1000, \n",
    "        **{k.split('__')[1]: v for k, v in best_params.items()},  \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Retrain the model on the entire training set with oversampling\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    best_rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using the provided eval function\n",
    "    accuracy, precision, recall, f1, roc_auc, pr_auc = utils.eval(y_test, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc\n",
    "    }\n",
    "\n",
    "    temp_result_list.append(metrics)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "accuracy_list = [result['accuracy'] for result in temp_result_list]\n",
    "precision_list = [result['precision'] for result in temp_result_list]\n",
    "recall_list = [result['recall'] for result in temp_result_list]\n",
    "f1_list = [result['f1'] for result in temp_result_list]\n",
    "roc_auc_list = [result['roc_auc'] for result in temp_result_list]\n",
    "pr_auc_list = [result['pr_auc'] for result in temp_result_list]\n",
    "\n",
    "metrics_summary = {\n",
    "    \"accuracy_mean\": np.mean(accuracy_list),\n",
    "    \"accuracy_std\": np.std(accuracy_list),\n",
    "    \"precision_mean\": np.mean(precision_list),\n",
    "    \"precision_std\": np.std(precision_list),\n",
    "    \"recall_mean\": np.mean(recall_list),\n",
    "    \"recall_std\": np.std(recall_list),\n",
    "    \"f1_mean\": np.mean(f1_list),\n",
    "    \"f1_std\": np.std(f1_list),\n",
    "    \"roc_auc_mean\": np.mean(roc_auc_list),\n",
    "    \"roc_auc_std\": np.std(roc_auc_list),\n",
    "    \"pr_auc_mean\": np.mean(pr_auc_list),\n",
    "    \"pr_auc_std\": np.std(pr_auc_list)\n",
    "}\n",
    "\n",
    "dict_save = {\n",
    "    \"weighting\": False,\n",
    "    \"oversampling\": True,\n",
    "    \"metrics_summary\": metrics_summary\n",
    "}\n",
    "\n",
    "path = \"C:/Users/Walraff/OneDrive - Universite de Liege/Documents/Ulg/Master2/TFE/Results/ValidProtocol\"\n",
    "utils.write_into_json(dict_save, f\"{path}/rf_scratch_experiment_3.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.2, 'max_depth': 1}\n",
      "0.4104033594580005\n",
      "Accuracy: 0.6774\n",
      "Precision: 0.3333\n",
      "Recall: 0.4286\n",
      "F1 Score: 0.3750\n",
      "ROC AUC: 0.5893\n",
      "PR AUC: 0.2719\n",
      "Fold: 1\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.01, 'max_depth': 3}\n",
      "0.36592512307713354\n",
      "Accuracy: 0.7419\n",
      "Precision: 0.3333\n",
      "Recall: 0.1429\n",
      "F1 Score: 0.2000\n",
      "ROC AUC: 0.5298\n",
      "PR AUC: 0.2412\n",
      "Fold: 2\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.3, 'max_depth': 3}\n",
      "0.39804043737480266\n",
      "Accuracy: 0.7742\n",
      "Precision: 0.6000\n",
      "Recall: 0.3750\n",
      "F1 Score: 0.4615\n",
      "ROC AUC: 0.6440\n",
      "PR AUC: 0.3863\n",
      "Fold: 3\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.4, 'max_depth': 1}\n",
      "0.386684581971027\n",
      "Accuracy: 0.6129\n",
      "Precision: 0.1667\n",
      "Recall: 0.1250\n",
      "F1 Score: 0.1429\n",
      "ROC AUC: 0.4538\n",
      "PR AUC: 0.2466\n",
      "Fold: 4\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.01, 'max_depth': 1}\n",
      "0.4540771756660503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7419\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "ROC AUC: 0.5000\n",
      "PR AUC: 0.2581\n",
      "Fold: 0\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.3, 'max_depth': 1}\n",
      "0.42218235743709764\n",
      "Accuracy: 0.6452\n",
      "Precision: 0.3333\n",
      "Recall: 0.5714\n",
      "F1 Score: 0.4211\n",
      "ROC AUC: 0.6190\n",
      "PR AUC: 0.2873\n",
      "Fold: 1\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.3, 'max_depth': 5}\n",
      "0.3887694296912996\n",
      "Accuracy: 0.7742\n",
      "Precision: 0.5000\n",
      "Recall: 0.4286\n",
      "F1 Score: 0.4615\n",
      "ROC AUC: 0.6518\n",
      "PR AUC: 0.3433\n",
      "Fold: 2\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.01, 'max_depth': 1}\n",
      "0.44045436045436037\n",
      "Accuracy: 0.5484\n",
      "Precision: 0.2857\n",
      "Recall: 0.5000\n",
      "F1 Score: 0.3636\n",
      "ROC AUC: 0.5326\n",
      "PR AUC: 0.2719\n",
      "Fold: 3\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.4, 'max_depth': 1}\n",
      "0.4305482589069546\n",
      "Accuracy: 0.6452\n",
      "Precision: 0.2000\n",
      "Recall: 0.1250\n",
      "F1 Score: 0.1538\n",
      "ROC AUC: 0.4755\n",
      "PR AUC: 0.2508\n",
      "Fold: 4\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'learning_rate': 0.01, 'max_depth': 5}\n",
      "0.40428201552215554\n",
      "Accuracy: 0.5806\n",
      "Precision: 0.2222\n",
      "Recall: 0.2500\n",
      "F1 Score: 0.2353\n",
      "ROC AUC: 0.4728\n",
      "PR AUC: 0.2491\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "count = 1\n",
    "\n",
    "# Trying with and without weighting\n",
    "for weighting in [False, True]:\n",
    "    temp_result_list = []\n",
    "\n",
    "    # loop over the folds\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(df['encoded_sequence'], df['quantotypic'])):\n",
    "        # Splitting the data into train and test sets\n",
    "        train_df = df.iloc[train_idx]\n",
    "        test_df = df.iloc[test_idx]\n",
    "        print(f\"Fold: {fold}\\n\")\n",
    "\n",
    "        # Prepare the data for training and testing\n",
    "        X_train = np.stack(train_df['encoded_sequence'].values)\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        y_train = train_df['quantotypic'].values\n",
    "\n",
    "        X_test = np.stack(test_df['encoded_sequence'].values)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "        y_test = test_df['quantotypic'].values\n",
    "\n",
    "        # Param grid for XGBClassifier\n",
    "        param_grid = {\n",
    "            'max_depth': [None, 1, 3, 5, 6],             \n",
    "            'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4], \n",
    "        }\n",
    "\n",
    "        # if weighting is True, add scale_pos_weight to the classifier\n",
    "        if weighting:\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=1000,\n",
    "                scale_pos_weight=pos_weight, \n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=1000,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "        # Train the model using GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grid, cv=5, scoring=\"average_precision\", verbose=4, n_jobs=-1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        print(grid_search.best_params_)\n",
    "        print(grid_search.best_score_)\n",
    "\n",
    "        # Retrieve the best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Initialize a new XGBClassifier with the best parameters\n",
    "        if weighting:\n",
    "            best_model = XGBClassifier(\n",
    "                n_estimators=1000,\n",
    "                scale_pos_weight=pos_weight,\n",
    "                random_state=42,\n",
    "                **best_params\n",
    "            )\n",
    "        else:\n",
    "            best_model = XGBClassifier(\n",
    "                n_estimators=1000,\n",
    "                random_state=42,\n",
    "                **best_params\n",
    "            )\n",
    "\n",
    "        # Retrain the model on the entire training set\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model using the provided eval function\n",
    "        accuracy, precision, recall, f1, roc_auc, pr_auc = utils.eval(y_test, y_pred)\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"pr_auc\": pr_auc\n",
    "        }\n",
    "        \n",
    "        temp_result_list.append(metrics)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "    accuracy_list = [result['accuracy'] for result in temp_result_list]\n",
    "    precision_list = [result['precision'] for result in temp_result_list]\n",
    "    recall_list = [result['recall'] for result in temp_result_list]\n",
    "    f1_list = [result['f1'] for result in temp_result_list]\n",
    "    roc_auc_list = [result['roc_auc'] for result in temp_result_list]\n",
    "    pr_auc_list = [result['pr_auc'] for result in temp_result_list]\n",
    "\n",
    "    metrics_summary = {\n",
    "        \"accuracy_mean\": np.mean(accuracy_list),\n",
    "        \"accuracy_std\": np.std(accuracy_list),\n",
    "        \"precision_mean\": np.mean(precision_list),\n",
    "        \"precision_std\": np.std(precision_list),\n",
    "        \"recall_mean\": np.mean(recall_list),\n",
    "        \"recall_std\": np.std(recall_list),\n",
    "        \"f1_mean\": np.mean(f1_list),\n",
    "        \"f1_std\": np.std(f1_list),\n",
    "        \"roc_auc_mean\": np.mean(roc_auc_list),\n",
    "        \"roc_auc_std\": np.std(roc_auc_list),\n",
    "        \"pr_auc_mean\": np.mean(pr_auc_list),\n",
    "        \"pr_auc_std\": np.std(pr_auc_list)\n",
    "    }\n",
    "\n",
    "    # Sauvegarde des rÃ©sultats\n",
    "    dict_save = {\n",
    "        \"weighting\": weighting,\n",
    "        \"oversampling\": False,\n",
    "        \"metrics_summary\": metrics_summary\n",
    "    }\n",
    "\n",
    "    path = \"C:/Users/Walraff/OneDrive - Universite de Liege/Documents/Ulg/Master2/TFE/Results/ValidProtocol\"\n",
    "    utils.write_into_json(dict_save, f\"{path}/xgb_scratch_experiment_{count}.json\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'classifier__learning_rate': 0.4, 'classifier__max_depth': 1}\n",
      "0.41843558029272315\n",
      "Accuracy: 0.6129\n",
      "Precision: 0.2727\n",
      "Recall: 0.4286\n",
      "F1 Score: 0.3333\n",
      "ROC AUC: 0.5476\n",
      "PR AUC: 0.2459\n",
      "Fold: 1\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'classifier__learning_rate': 0.1, 'classifier__max_depth': 1}\n",
      "0.3952860812748463\n",
      "Accuracy: 0.7419\n",
      "Precision: 0.4000\n",
      "Recall: 0.2857\n",
      "F1 Score: 0.3333\n",
      "ROC AUC: 0.5804\n",
      "PR AUC: 0.2756\n",
      "Fold: 2\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'classifier__learning_rate': 0.01, 'classifier__max_depth': 1}\n",
      "0.41908160608927875\n",
      "Accuracy: 0.6129\n",
      "Precision: 0.3333\n",
      "Recall: 0.5000\n",
      "F1 Score: 0.4000\n",
      "ROC AUC: 0.5761\n",
      "PR AUC: 0.2957\n",
      "Fold: 3\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'classifier__learning_rate': 0.2, 'classifier__max_depth': 1}\n",
      "0.42490841847363586\n",
      "Accuracy: 0.6774\n",
      "Precision: 0.3333\n",
      "Recall: 0.2500\n",
      "F1 Score: 0.2857\n",
      "ROC AUC: 0.5380\n",
      "PR AUC: 0.2769\n",
      "Fold: 4\n",
      "\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "{'classifier__learning_rate': 0.01, 'classifier__max_depth': 3}\n",
      "0.3872928395868899\n",
      "Accuracy: 0.5806\n",
      "Precision: 0.2222\n",
      "Recall: 0.2500\n",
      "F1 Score: 0.2353\n",
      "ROC AUC: 0.4728\n",
      "PR AUC: 0.2491\n"
     ]
    }
   ],
   "source": [
    "# New loop with oversampling using ImbPipeline\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "temp_result_list = []\n",
    "# loop over folds\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(df['encoded_sequence'], df['quantotypic'])):\n",
    "    # Splitting the data into train and test sets\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    print(f\"Fold: {fold}\\n\")\n",
    "\n",
    "    # Prepare the data for training and testing\n",
    "    X_train = np.stack(train_df['encoded_sequence'].values)\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    y_train = train_df['quantotypic'].values\n",
    "\n",
    "    X_test = np.stack(test_df['encoded_sequence'].values)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    y_test = test_df['quantotypic'].values\n",
    "    \n",
    "    # Param grid for XGBClassifier\n",
    "    param_grid = {\n",
    "        'classifier__max_depth': [None, 1, 3, 5, 6],  \n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],  \n",
    "    }\n",
    "\n",
    "    # Create a pipeline with RandomOverSampler and XGBClassifier\n",
    "    pipeline = ImbPipeline([\n",
    "        ('oversampler', RandomOverSampler(random_state=42)), \n",
    "        ('classifier', XGBClassifier(\n",
    "            n_estimators=1000, \n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Training the model using GridSearchCV\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,  # Validation croisÃ©e\n",
    "        scoring='average_precision',\n",
    "        verbose=4,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "    print(grid_search_xgb.best_params_)\n",
    "    print(grid_search_xgb.best_score_)\n",
    "\n",
    "    # Retrieve the best parameters\n",
    "    best_params = {k.split('__')[1]: v for k, v in grid_search_xgb.best_params_.items()}\n",
    "\n",
    "    # Retrain the model on the entire training set with oversampling\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    best_xgb = XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        random_state=42,\n",
    "        **best_params  \n",
    "    )\n",
    "\n",
    "    best_xgb.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using the provided eval function\n",
    "    accuracy, precision, recall, f1, roc_auc, pr_auc = utils.eval(y_test, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc\n",
    "    }\n",
    "\n",
    "    temp_result_list.append(metrics)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "accuracy_list = [result['accuracy'] for result in temp_result_list]\n",
    "precision_list = [result['precision'] for result in temp_result_list]\n",
    "recall_list = [result['recall'] for result in temp_result_list]\n",
    "f1_list = [result['f1'] for result in temp_result_list]\n",
    "roc_auc_list = [result['roc_auc'] for result in temp_result_list]\n",
    "pr_auc_list = [result['pr_auc'] for result in temp_result_list]\n",
    "\n",
    "metrics_summary = {\n",
    "    \"accuracy_mean\": np.mean(accuracy_list),\n",
    "    \"accuracy_std\": np.std(accuracy_list),\n",
    "    \"precision_mean\": np.mean(precision_list),\n",
    "    \"precision_std\": np.std(precision_list),\n",
    "    \"recall_mean\": np.mean(recall_list),\n",
    "    \"recall_std\": np.std(recall_list),\n",
    "    \"f1_mean\": np.mean(f1_list),\n",
    "    \"f1_std\": np.std(f1_list),\n",
    "    \"roc_auc_mean\": np.mean(roc_auc_list),\n",
    "    \"roc_auc_std\": np.std(roc_auc_list),\n",
    "    \"pr_auc_mean\": np.mean(pr_auc_list),\n",
    "    \"pr_auc_std\": np.std(pr_auc_list)\n",
    "}\n",
    "\n",
    "dict_save = {\n",
    "    \"weighting\": False,\n",
    "    \"oversampling\": True,\n",
    "    \"metrics_summary\": metrics_summary\n",
    "}\n",
    "\n",
    "path = \"C:/Users/Walraff/OneDrive - Universite de Liege/Documents/Ulg/Master2/TFE/Results/ValidProtocol\"\n",
    "utils.write_into_json(dict_save, f\"{path}/xgb_scratch_experiment_3.json\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
